{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-16T01:33:21.470771Z",
     "iopub.status.busy": "2025-01-16T01:33:21.470465Z",
     "iopub.status.idle": "2025-01-16T01:45:11.294908Z",
     "shell.execute_reply": "2025-01-16T01:45:11.294105Z",
     "shell.execute_reply.started": "2025-01-16T01:33:21.470747Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 13ms/step - accuracy: 0.3895 - loss: 1.7261 - val_accuracy: 0.5600 - val_loss: 1.2706 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.5368 - loss: 1.3223 - val_accuracy: 0.5825 - val_loss: 1.2023 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.5562 - loss: 1.2568 - val_accuracy: 0.5952 - val_loss: 1.1686 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.5780 - loss: 1.2118 - val_accuracy: 0.6002 - val_loss: 1.1414 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.5902 - loss: 1.1805 - val_accuracy: 0.6017 - val_loss: 1.1311 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.5907 - loss: 1.1680 - val_accuracy: 0.6025 - val_loss: 1.1160 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.5975 - loss: 1.1454 - val_accuracy: 0.6043 - val_loss: 1.1174 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6031 - loss: 1.1280 - val_accuracy: 0.6092 - val_loss: 1.1037 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6140 - loss: 1.1062 - val_accuracy: 0.6125 - val_loss: 1.0999 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6147 - loss: 1.1071 - val_accuracy: 0.6160 - val_loss: 1.0871 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6202 - loss: 1.0834 - val_accuracy: 0.6230 - val_loss: 1.0880 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.6267 - loss: 1.0717 - val_accuracy: 0.6190 - val_loss: 1.0908 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.6261 - loss: 1.0670 - val_accuracy: 0.6185 - val_loss: 1.0909 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.6329 - loss: 1.0460 - val_accuracy: 0.6180 - val_loss: 1.0828 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6363 - loss: 1.0445 - val_accuracy: 0.6188 - val_loss: 1.0840 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.6371 - loss: 1.0397 - val_accuracy: 0.6183 - val_loss: 1.0816 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6414 - loss: 1.0206 - val_accuracy: 0.6228 - val_loss: 1.0786 - learning_rate: 0.0010\n",
      "Epoch 18/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6430 - loss: 1.0087 - val_accuracy: 0.6260 - val_loss: 1.0706 - learning_rate: 0.0010\n",
      "Epoch 19/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6425 - loss: 1.0038 - val_accuracy: 0.6210 - val_loss: 1.0685 - learning_rate: 0.0010\n",
      "Epoch 20/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6506 - loss: 0.9935 - val_accuracy: 0.6227 - val_loss: 1.0766 - learning_rate: 0.0010\n",
      "Epoch 21/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6505 - loss: 0.9816 - val_accuracy: 0.6245 - val_loss: 1.0772 - learning_rate: 0.0010\n",
      "Epoch 22/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6550 - loss: 0.9790 - val_accuracy: 0.6232 - val_loss: 1.0745 - learning_rate: 0.0010\n",
      "Epoch 23/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6523 - loss: 0.9812 - val_accuracy: 0.6270 - val_loss: 1.0710 - learning_rate: 0.0010\n",
      "Epoch 24/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6587 - loss: 0.9733 - val_accuracy: 0.6235 - val_loss: 1.0710 - learning_rate: 0.0010\n",
      "Epoch 25/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6726 - loss: 0.9305 - val_accuracy: 0.6295 - val_loss: 1.0636 - learning_rate: 5.0000e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6762 - loss: 0.9233 - val_accuracy: 0.6320 - val_loss: 1.0645 - learning_rate: 5.0000e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6800 - loss: 0.9100 - val_accuracy: 0.6313 - val_loss: 1.0638 - learning_rate: 5.0000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6815 - loss: 0.9086 - val_accuracy: 0.6325 - val_loss: 1.0610 - learning_rate: 5.0000e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6803 - loss: 0.9033 - val_accuracy: 0.6315 - val_loss: 1.0607 - learning_rate: 5.0000e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.6840 - loss: 0.8926 - val_accuracy: 0.6305 - val_loss: 1.0649 - learning_rate: 5.0000e-04\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.6358 - loss: 1.0734\n",
      "Test Accuracy: 63.60%\n",
      "Epoch 1/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 45ms/step - accuracy: 0.5718 - loss: 1.2982 - val_accuracy: 0.7538 - val_loss: 0.7274 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - accuracy: 0.7986 - loss: 0.6011 - val_accuracy: 0.8128 - val_loss: 0.5329 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - accuracy: 0.8517 - loss: 0.4357 - val_accuracy: 0.8378 - val_loss: 0.4974 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - accuracy: 0.8958 - loss: 0.3194 - val_accuracy: 0.8468 - val_loss: 0.4610 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - accuracy: 0.9189 - loss: 0.2466 - val_accuracy: 0.8350 - val_loss: 0.5202 - learning_rate: 1.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - accuracy: 0.9336 - loss: 0.1982 - val_accuracy: 0.8395 - val_loss: 0.5503 - learning_rate: 1.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - accuracy: 0.9541 - loss: 0.1378 - val_accuracy: 0.8452 - val_loss: 0.5879 - learning_rate: 1.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - accuracy: 0.9637 - loss: 0.1096 - val_accuracy: 0.8560 - val_loss: 0.5977 - learning_rate: 1.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - accuracy: 0.9721 - loss: 0.0886 - val_accuracy: 0.8562 - val_loss: 0.6229 - learning_rate: 1.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - accuracy: 0.9912 - loss: 0.0292 - val_accuracy: 0.8717 - val_loss: 0.7173 - learning_rate: 5.0000e-05\n",
      "Epoch 11/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - accuracy: 0.9954 - loss: 0.0141 - val_accuracy: 0.8653 - val_loss: 0.7123 - learning_rate: 5.0000e-05\n",
      "Epoch 12/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - accuracy: 0.9946 - loss: 0.0173 - val_accuracy: 0.8718 - val_loss: 0.6769 - learning_rate: 5.0000e-05\n",
      "Epoch 13/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - accuracy: 0.9953 - loss: 0.0164 - val_accuracy: 0.8658 - val_loss: 0.8334 - learning_rate: 5.0000e-05\n",
      "Epoch 14/30\n",
      "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 43ms/step - accuracy: 0.9936 - loss: 0.0217 - val_accuracy: 0.8665 - val_loss: 0.8095 - learning_rate: 5.0000e-05\n",
      "\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8390 - loss: 0.4800\n",
      "Final Test Accuracy after Fine-Tuning: 84.02%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess CIFAR-10 data\n",
    "(X_train_keras, y_train_keras), (X_test_keras, y_test_keras) = cifar10.load_data()\n",
    "X = np.concatenate((X_train_keras, X_test_keras), axis=0)\n",
    "y = np.concatenate((y_train_keras, y_test_keras), axis=0)\n",
    "X_scaled = X / 255.0\n",
    "y_cat = to_categorical(y, 10)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train, X_remain, y_train, y_remain = train_test_split(X_scaled, y_cat, test_size=0.2, random_state=42, stratify=y_cat)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_remain, y_remain, test_size=0.5, random_state=42, stratify=y_remain)\n",
    "\n",
    "# Load pre-trained VGG16 without the top layer\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "# Add custom classification layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Create the complete model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Freeze the base model layers to retain pre-trained weights\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks for early stopping and learning rate reduction\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "# Fine-tune the model by unfreezing some layers of the base model\n",
    "for layer in base_model.layers[-80:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile the model with a lower learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train again with fine-tuning\n",
    "fine_tune_history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=64,\n",
    "    epochs=30,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "# Final evaluation\n",
    "final_loss, final_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Final Test Accuracy after Fine-Tuning: {final_accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
